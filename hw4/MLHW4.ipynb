{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71alAabAjVfY"
   },
   "source": [
    "\n",
    "# **<font color='##6ee05a'>Machine Learning Homework 4</font>**\n",
    "\n",
    "*   Archisa Bhattacharya\n",
    "*   Marz Pino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3Iy1aTXdiiD"
   },
   "source": [
    "# **<font color='violet'>Part 1:</font> Loading the Data, EDA, & Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbtxqRAsljnY"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g_bRAcxUZpkq"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq48FnIwdj4r"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  label\n",
      "0  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      5\n",
      "1  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      0\n",
      "2  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      4\n",
      "3  {'bytes': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      1\n",
      "4  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      9\n"
     ]
    }
   ],
   "source": [
    "print(data.df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-o9MhzZlbgP"
   },
   "source": [
    "# **1. [80 points, LeNet5 Implementation]** Please implement the original LetNet5 exactly as described in the paper “Gradient-Based Learning Applied to Document Recognition”. Please carefully read the Sections I, II, III and Appendix A, B, C. For your implementation, you can use PyTorch or TensorFlow. (PyTorch is recommended if you don’t have any specific preference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0qicca2mUTZ"
   },
   "source": [
    "**1.1 [Architecture]** Implement LeNet architecture as described in Section II. For the RBF parameters, use the data DIGIT. There are several ways to generate the 7 x 12 bitmap image for each\n",
    "digit from the data. Please explain your approach and reasoning behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m df_train, df_test \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Define transformations\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     70\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mGrayscale(),\n\u001b[1;32m     71\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     72\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))\n\u001b[1;32m     73\u001b[0m ])\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Create datasets and dataloaders\u001b[39;00m\n\u001b[1;32m     76\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m MNISTDataset(df_train, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/__init__.py:1938\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # Layer 1: Convolutional layer (C1)\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        # Layer 2: Subsampling (S2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # Layer 3: Convolutional layer (C3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # Layer 4: Subsampling (S4)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # Layer 5: Convolutional layer (C5, fully connected)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)\n",
    "        # Layer 6: Fully connected (F6)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layers sequentially\n",
    "        x = 1.7159 * torch.tanh(self.conv1(x) * 2 / 3)  # C1\n",
    "        x = self.pool1(x)  # S2\n",
    "        x = 1.7159 * torch.tanh(self.conv2(x) * 2 / 3)  # C3\n",
    "        x = self.pool2(x)  # S4\n",
    "        x = 1.7159 * torch.tanh(self.conv3(x) * 2 / 3)  # C5\n",
    "        x = x.view(-1, 120)  # Flatten\n",
    "        x = 1.7159 * torch.tanh(self.fc1(x) * 2 / 3)  # F6\n",
    "        x = self.fc2(x)  # Output\n",
    "        return x\n",
    "\n",
    "# Define custom dataset class\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(self.data.iloc[idx, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "        return image, label\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data():\n",
    "    splits = {'train': 'mnist/train-00000-of-00001.parquet', 'test': 'mnist/test-00000-of-00001.parquet'}\n",
    "    df_train = pd.read_parquet(\"hf://datasets/ylecun/mnist/\" + splits[\"train\"])\n",
    "    df_test = pd.read_parquet(\"hf://datasets/ylecun/mnist/\" + splits[\"test\"])\n",
    "    return df_train, df_test\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    df_train, df_test = load_data()\n",
    "\n",
    "    # Define transformations\n",
    "    transform = torch.transforms.Compose([\n",
    "        torch.transforms.Grayscale(),\n",
    "        torch.transforms.ToTensor(),\n",
    "        torch.transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = MNISTDataset(df_train, transform=transform)\n",
    "    test_dataset = MNISTDataset(df_test, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = LeNet5()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3461616.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 3600193.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 17032453.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4761441.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                        train = True,\n",
    "                                        transform = transforms.Compose([\n",
    "                                                transforms.Resize((32,32)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                        download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                        train = False,\n",
    "                                        transform = transforms.Compose([\n",
    "                                                transforms.Resize((32,32)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                        download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LeNet5.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLeNet5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Setting the loss function\u001b[39;00m\n\u001b[1;32m      4\u001b[0m cost \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: LeNet5.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "    \n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.3005\n",
      "Test Accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # LeNet-5 original structure:\n",
    "        # Input: 1x28x28 (grayscale)\n",
    "        # C1: Conv2D -> 6x24x24\n",
    "        # S2: AvgPool -> 6x12x12\n",
    "        # C3: Conv2D -> 16x8x8\n",
    "        # S4: AvgPool -> 16x4x4\n",
    "        # C5: Conv2D -> 120x1x1 (fully connected as conv)\n",
    "        # F6: 84 units fully connected\n",
    "        # Output: 10 units fully connected (digits 0-9)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)   # (N,1,28,28)->(N,6,24,24)\n",
    "        self.pool = nn.AvgPool2d(2)                    # (N,6,24,24)->(N,6,12,12)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)   # (N,6,12,12)->(N,16,8,8)\n",
    "                                                       # pool -> (N,16,4,4)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)              # (N,16*4*4)->(N,120)\n",
    "        self.fc2 = nn.Linear(120, 84)                  # (N,120)->(N,84)\n",
    "        self.fc3 = nn.Linear(84, 10)                   # (N,84)->(N,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16*4*4)  # Flatten before FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # raw logits for 10 classes\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up transform for MNIST: to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load MNIST dataset from torchvision\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LeNet5().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Test Accuracy:\", correct / total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHwm5-vbmclf"
   },
   "source": [
    "**1.2 [Training]**\n",
    "*   Data: Run data.py to collect MNIST data samples. The MNIST database consists of 28 × 28 images. You\n",
    "will need to resize them to 32 × 32 to match the regular database in the original paper.\n",
    "*   Optimization: Use the “Stochastic Diagonal Levenberg-Marquardt” method. Compute the exact second\n",
    "derivative for hkk in equation (22) without the approximations in Appendix C.\n",
    "*  Loss function: Use equation (9). j = 0.1 and i denotes incorrect classes.\n",
    "* At every pass (every epoch), track the error rates. You will need the rates for plotting in the performance\n",
    "evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUaC98y4vW2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWLQbVwVvohm"
   },
   "source": [
    "**1.3 [Performance Evaluation]**\n",
    "* Create a plot of training and test error rate, similar to Fig 5.\n",
    "* Compute a confusion matrix (10 × 10).\n",
    "* For each digit, identify the most confusing example. For instance, what image was misclassified with the highest confidence as the digit “one”?\n",
    "* Report your train and test error rates at epoch 20."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
